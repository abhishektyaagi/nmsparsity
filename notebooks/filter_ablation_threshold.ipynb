{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "\n",
    "with hydra.initialize(config_path=\"../configs\"):\n",
    "    cfg = hydra.compose(config_name=\"config.yaml\", overrides=[\"dataset=cifar10\", \"model=wide_resnet22\", \"compute.distributed=False\"])\n",
    "cfg\n",
    "\n",
    "net = ModelFactory.load_model(\"wide_resnet22\", \"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_loader, test_loader = get_dataloaders(cfg)\n",
    "model = ModelFactory.load_model(\n",
    "        model=cfg.model.name, dataset=cfg.dataset.name\n",
    "    )\n",
    "model.to(device)\n",
    "optimizer = get_optimizer(cfg, model, state_dict=None)\n",
    "scheduler = get_lr_scheduler(cfg, optimizer, state_dict=None)\n",
    "\n",
    "def get_T_end(cfg) -> int:\n",
    "    \"\"\"Get step number to terminate pruning / regrowth based on cfg settings.\n",
    "\n",
    "    Args:\n",
    "        cfg : _description_\n",
    "\n",
    "    Returns:\n",
    "        int: _description_\n",
    "    \"\"\"\n",
    "    if cfg.training.max_steps is None:\n",
    "        if cfg.compute.distributed:\n",
    "            # In distributed mode, len(train_loader) will be reduced by\n",
    "            # 1/world_size compared to single device\n",
    "            T_end = int(\n",
    "                0.75\n",
    "                * cfg.training.epochs\n",
    "                * len(train_loader)  # Dataset length // batch_size\n",
    "                * cfg.compute.world_size\n",
    "            )\n",
    "        else:\n",
    "            T_end = int(0.75 * cfg.training.epochs * len(train_loader))\n",
    "    else:\n",
    "        T_end = int(0.75 * cfg.training.max_steps)\n",
    "    if not cfg.rigl.use_t_end:\n",
    "        T_end = int(1 / 0.75 * T_end)  # We use the full number of steps\n",
    "    return T_end"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
