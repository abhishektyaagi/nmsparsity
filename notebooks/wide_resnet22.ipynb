{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5057/1328889562.py:32: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with hydra.initialize(config_path=\"../configs\"):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': {'name': 'cifar10', 'normalize': False, 'num_classes': 10, 'classes': ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'train_len': 50000}, 'model': {'name': 'wide_resnet22', 'activation_function': 'relu'}, 'experiment': {'comment': 'dense_alloc-${rigl.dense_allocation}_const_fan_in-${rigl.const_fan_in}_weight_per_neuron-${rigl.min_salient_weights_per_neuron}_sparse_init-True-torch-init', 'name': '${model.name}_${dataset.name}_${experiment.comment}', 'resume_from_checkpoint': False, 'run_id': None}, 'paths': {'base': '${oc.env:BASE_PATH}', 'data_folder': '${paths.base}/data', 'artifacts': '${paths.base}/artifacts', 'logs': '${paths.base}/logs', 'checkpoints': '${paths.artifacts}/checkpoints'}, 'rigl': {'dense_allocation': 0.01, 'delta': 2, 'grad_accumulation_n': 1, 'alpha': 0.3, 'static_topo': 0, 'const_fan_in': True, 'sparsity_distribution': 'erk', 'erk_power_scale': 1.0, 'use_t_end': True, 'static_ablation': False, 'dynamic_ablation': True, 'filter_ablation_threshold': 0.01, 'use_sparse_initialization': True, 'init_method_str': 'sparse torch', 'min_salient_weights_per_neuron': 1, 'ignore_linear_layers': False}, 'training': {'dry_run': False, 'batch_size': 128, 'simulated_batch_size': None, 'test_batch_size': 1000, 'epochs': 1, 'seed': 42, 'log_interval': 1000, 'save_model': True, 'max_steps': None, 'optimizer': 'sgd', 'weight_decay': 0.0005, 'momentum': 0.9, 'label_smoothing': 0.0, 'scheduler': 'step_lr', 'lr': 0.1, 'init_lr': 1e-06, 'warm_up_steps': 0, 'gamma': 0.2, 'step_size': 77}, 'compute': {'no_cuda': False, 'cuda_kwargs': {'num_workers': '${ oc.decode:${oc.env:NUM_WORKERS} }', 'pin_memory': True}, 'distributed': False, 'world_size': 4, 'dist_backend': 'nccl'}, 'wandb': {'project': 'condensed-rigl', 'entity': 'condensed-sparsity', 'start_method': 'thread', 'log_images': False, 'watch_model_grad_and_weights': False, 'log_filter_stats': True}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import dotenv\n",
    "import omegaconf\n",
    "import hydra\n",
    "import logging\n",
    "import wandb\n",
    "from datetime import date\n",
    "import pathlib\n",
    "from typing import Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from rigl_torch.models.model_factory import ModelFactory\n",
    "from rigl_torch.rigl_scheduler import RigLScheduler\n",
    "from rigl_torch.rigl_constant_fan import RigLConstFanScheduler\n",
    "from rigl_torch.datasets import get_dataloaders\n",
    "from rigl_torch.optim import (\n",
    "    get_optimizer,\n",
    "    get_lr_scheduler,\n",
    ")\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.utils.rigl_utils import get_T_end, get_fan_in_after_ablation, get_conv_idx_from_flat_idx\n",
    "\n",
    "dataset = \"cifar10\"\n",
    "model=\"wide_resnet22\"\n",
    "with hydra.initialize(config_path=\"../configs\"):\n",
    "    cfg = hydra.compose(\n",
    "        config_name=\"config.yaml\", \n",
    "        overrides=[\n",
    "            f\"dataset={dataset}\",\n",
    "            f\"model={model}\",\n",
    "            \"compute.distributed=False\",\n",
    "            \"rigl.dense_allocation=0.01\",\n",
    "            \"rigl.const_fan_in=True\",\n",
    "            \"rigl.filter_ablation_threshold=0.01\",\n",
    "            \"rigl.dynamic_ablation=True\", \n",
    "            \"rigl.static_ablation=False\",\n",
    "            \"rigl.min_salient_weights_per_neuron=1\",\n",
    "            \"rigl.delta=2\",\n",
    "            \"rigl.grad_accumulation_n=1\",\n",
    "        ]\n",
    "    )\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/user/condensed-sparsity/src/rigl_torch/models/model_factory.py:Loading model wide_resnet22/cifar10 using <function get_wide_resnet_22 at 0x7f5cb2ba51f0> with args: () and kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "train_loader, test_loader = get_dataloaders(cfg)\n",
    "model = ModelFactory.load_model(\n",
    "        model=cfg.model.name, dataset=cfg.dataset.name\n",
    "    )\n",
    "model.to(device)\n",
    "optimizer = get_optimizer(cfg, model, state_dict=None)\n",
    "scheduler = get_lr_scheduler(cfg, optimizer, state_dict=None)\n",
    "T_end = get_T_end(cfg, train_loader)\n",
    "if cfg.rigl.const_fan_in:\n",
    "    rigl_scheduler = RigLConstFanScheduler\n",
    "else:\n",
    "    rigl_scheduler = RigLScheduler\n",
    "pruner = rigl_scheduler(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dense_allocation=cfg.rigl.dense_allocation,\n",
    "    alpha=cfg.rigl.alpha,\n",
    "    delta=cfg.rigl.delta,\n",
    "    static_topo=cfg.rigl.static_topo,\n",
    "    T_end=T_end,\n",
    "    ignore_linear_layers=False,\n",
    "    grad_accumulation_n=cfg.rigl.grad_accumulation_n,\n",
    "    sparsity_distribution=cfg.rigl.sparsity_distribution,\n",
    "    erk_power_scale=cfg.rigl.erk_power_scale,\n",
    "    state_dict=None,\n",
    "    filter_ablation_threshold=cfg.rigl.filter_ablation_threshold,\n",
    "    static_ablation=cfg.rigl.static_ablation,\n",
    "    dynamic_ablation=cfg.rigl.dynamic_ablation,\n",
    "    min_salient_weights_per_neuron=cfg.rigl.min_salient_weights_per_neuron,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rigl_torch.utils.rigl_utils import get_W, get_names_and_W\n",
    "\n",
    "W = get_W(model, False)\n",
    "len(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, W_prime = get_names_and_W(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert W == W_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1',\n",
       " 'block1.layer.0.conv1',\n",
       " 'block1.layer.0.conv2',\n",
       " 'block1.layer.0.convShortcut',\n",
       " 'block1.layer.1.conv1',\n",
       " 'block1.layer.1.conv2',\n",
       " 'block1.layer.2.conv1',\n",
       " 'block1.layer.2.conv2',\n",
       " 'block2.layer.0.conv1',\n",
       " 'block2.layer.0.conv2',\n",
       " 'block2.layer.0.convShortcut',\n",
       " 'block2.layer.1.conv1',\n",
       " 'block2.layer.1.conv2',\n",
       " 'block2.layer.2.conv1',\n",
       " 'block2.layer.2.conv2',\n",
       " 'block3.layer.0.conv1',\n",
       " 'block3.layer.0.conv2',\n",
       " 'block3.layer.0.convShortcut',\n",
       " 'block3.layer.1.conv1',\n",
       " 'block3.layer.1.conv2',\n",
       " 'block3.layer.2.conv1',\n",
       " 'block3.layer.2.conv2',\n",
       " 'fc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "816b48dc46e0e4033a4b7ddacb526e2f216437e7413cf9fdf092ed7be3b64e38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
