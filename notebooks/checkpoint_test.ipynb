{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import dotenv\n",
    "import omegaconf\n",
    "from datetime import datetime\n",
    "import hydra\n",
    "import logging\n",
    "import wandb\n",
    "from datetime import date\n",
    "import pathlib\n",
    "from typing import Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "\n",
    "from rigl_torch.models.model_factory import ModelFactory\n",
    "from rigl_torch.rigl_scheduler import RigLScheduler\n",
    "from rigl_torch.rigl_constant_fan import RigLConstFanScheduler\n",
    "from rigl_torch.datasets import get_dataloaders\n",
    "from rigl_torch.optim import (\n",
    "    get_optimizer,\n",
    "    get_lr_scheduler,\n",
    ")\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.utils.rigl_utils import get_T_end\n",
    "from rigl_torch.meters import TrainingMeter\n",
    "from rigl_torch.utils.wandb_utils import WandbRunName, wandb_log_check\n",
    "\n",
    "\n",
    "def _get_checkpoint(cfg: omegaconf.DictConfig, rank: int, logger) -> Checkpoint:\n",
    "    run_id = cfg.experiment.run_id\n",
    "    if run_id is None:\n",
    "        raise ValueError(\n",
    "            \"Must provide wandb run_id when \"\n",
    "            \"cfg.training.resume_from_checkpoint is True\"\n",
    "        )\n",
    "    checkpoint = Checkpoint.load_last_checkpoint(\n",
    "        run_id=run_id,\n",
    "        parent_dir=cfg.paths.checkpoints,\n",
    "        rank=rank,\n",
    "    )\n",
    "    if checkpoint.run_id != run_id:\n",
    "        logger.warning(f\"Mismatched run_id found! {checkpoint.run_id} != {run_id}\")\n",
    "        checkpoint.run_id = run_id\n",
    "    logger.info(f\"Resuming training with run_id: {cfg.experiment.run_id}\")\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def init_wandb(cfg: omegaconf.DictConfig, wandb_init_kwargs: Dict[str, Any]):\n",
    "    # We override logging functions now to avoid any calls\n",
    "    if not cfg.wandb.log_to_wandb:\n",
    "        print(\"No logging to WANDB! See cfg.wandb.log_to_wandb\")\n",
    "        wandb.log = wandb_log_check(wandb.log, cfg.wandb.log_to_wandb)\n",
    "        wandb.log_artifact = wandb_log_check(\n",
    "            wandb.log_artifact, cfg.wandb.log_to_wandb\n",
    "        )\n",
    "        wandb.watch = wandb_log_check(wandb.watch, cfg.wandb.log_to_wandb)\n",
    "        return None\n",
    "    _ = WandbRunName(name=cfg.experiment.name)  # Verify name is OK\n",
    "    run = wandb.init(\n",
    "        name=cfg.experiment.name,\n",
    "        entity=cfg.wandb.entity,\n",
    "        project=cfg.wandb.project,\n",
    "        config=omegaconf.OmegaConf.to_container(\n",
    "            cfg=cfg, resolve=True, throw_on_missing=True\n",
    "        ),\n",
    "        settings=wandb.Settings(start_method=cfg.wandb.start_method),\n",
    "        **wandb_init_kwargs,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "@hydra.main(config_path=\"configs/\", config_name=\"config\", version_base=\"1.2\")\n",
    "def initalize_main(cfg: omegaconf.DictConfig) -> None:\n",
    "    use_cuda = not cfg.compute.no_cuda and torch.cuda.is_available()\n",
    "    if not use_cuda:\n",
    "        raise SystemError(\"GPU has stopped responding...waiting to die!\")\n",
    "    if cfg.compute.distributed:\n",
    "        # We initalize train and val loaders here to ensure .tar balls have\n",
    "        # been decompressed before parallel workers try and write the same\n",
    "        # directories!\n",
    "        single_proc_cfg = deepcopy(cfg)\n",
    "        single_proc_cfg.compute.distributed = False\n",
    "        train_loader, test_loader = get_dataloaders(single_proc_cfg)\n",
    "        del train_loader\n",
    "        del test_loader\n",
    "        del single_proc_cfg\n",
    "        wandb.setup()\n",
    "        mp.spawn(\n",
    "            main,\n",
    "            args=(cfg,),\n",
    "            nprocs=cfg.compute.world_size,\n",
    "        )\n",
    "    else:\n",
    "        main(0, cfg)  # Single GPU\n",
    "\n",
    "\n",
    "def _get_logger(rank, cfg: omegaconf.DictConfig) -> logging.Logger:\n",
    "    log_path = pathlib.Path(cfg.paths.logs)\n",
    "    if not log_path.is_dir():\n",
    "        log_path.mkdir()\n",
    "    logger = logging.getLogger(__file__)\n",
    "    logger.setLevel(level=logging.INFO)\n",
    "    current_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "    # logformat = \"[%(levelname)s] %(asctime)s G- %(name)s -%(rank)s -\n",
    "    # %(funcName)s (%(lineno)d) : %(message)s\"\n",
    "    logformat = (\n",
    "        \"[%(levelname)s] %(asctime)s G- %(name)s - %(funcName)s \"\n",
    "        \"(%(lineno)d) : %(message)s\"\n",
    "    )\n",
    "    logging.root.handlers = []\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=logformat,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path / f\"processor_{current_date}.log\"),\n",
    "            logging.StreamHandler(),\n",
    "        ],\n",
    "    )\n",
    "    # logger = logging.LoggerAdapter(logger, {\"rank\": f\"rank: {rank}\"})\n",
    "    # logger.info(\"hell world\")\n",
    "    return logger\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "import os\n",
    "with initialize(\"../configs\", version_base=\"1.2.0\"):\n",
    "    cfg = compose(\n",
    "        \"config.yaml\",\n",
    "        overrides=[\n",
    "            \"dataset=imagenet\",\n",
    "            \"compute.distributed=False\",\n",
    "            \"model=resnet50\"\n",
    "            ])\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "print(cfg.model.name)\n",
    "rank=0\n",
    "\n",
    "\n",
    "logger = _get_logger(rank, cfg)\n",
    "if cfg.experiment.resume_from_checkpoint:\n",
    "    checkpoint = _get_checkpoint(cfg, rank, logger)\n",
    "    wandb_init_resume = \"must\"\n",
    "    run_id = checkpoint.run_id\n",
    "    cfg = checkpoint.cfg\n",
    "    cfg.experiment.run_id = run_id\n",
    "    cfg.experiment.resume_from_checkpoint = True\n",
    "else:\n",
    "    run_id = None\n",
    "    wandb_init_resume = None\n",
    "    checkpoint = None\n",
    "logger.info(f\"Running train_rigl.py with config:\\n{cfg}\")\n",
    "\n",
    "if cfg.compute.distributed:\n",
    "    dist.init_process_group(\n",
    "        backend=cfg.compute.dist_backend,\n",
    "        world_size=cfg.compute.world_size,\n",
    "        rank=rank,\n",
    "    )\n",
    "run_id, optimizer_state, scheduler_state, pruner_state, model_state = (\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    ")\n",
    "\n",
    "if checkpoint is not None:\n",
    "    run_id = checkpoint.run_id\n",
    "    optimizer_state = checkpoint.optimizer\n",
    "    scheduler_state = checkpoint.scheduler\n",
    "    pruner_state = checkpoint.pruner\n",
    "    model_state = checkpoint.model\n",
    "    logger.info(f\"Resuming training with run_id: {run_id}\")\n",
    "    cfg = checkpoint.cfg\n",
    "\n",
    "if rank == 0:\n",
    "    wandb_init_kwargs = dict(resume=wandb_init_resume, id=run_id)\n",
    "    run = init_wandb(cfg, wandb_init_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "832f61c19470ea428b2cef022cd1fe1aa91b00b83b99363aeeaecf593912d607"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
