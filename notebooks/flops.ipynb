{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import dotenv\n",
    "import omegaconf\n",
    "import hydra\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import wandb\n",
    "from datetime import date\n",
    "import dotenv\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from rigl_torch.models import ModelFactory\n",
    "from rigl_torch.rigl_scheduler import RigLScheduler\n",
    "from rigl_torch.rigl_constant_fan import RigLConstFanScheduler\n",
    "from rigl_torch.datasets import get_dataloaders\n",
    "from rigl_torch.optim import (\n",
    "    get_optimizer,\n",
    "    get_lr_scheduler,\n",
    ")\n",
    "from rigl_torch.utils.rigl_utils import get_names_and_W\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.utils.rigl_utils import get_T_end, get_fan_in_after_ablation, get_conv_idx_from_flat_idx\n",
    "from hydra import initialize, compose\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruner_model_loader(dense_alloc, model, dataset):\n",
    "    with initialize(\"../configs\", version_base=\"1.2.0\"):\n",
    "        cfg = compose(\n",
    "            \"config.yaml\",\n",
    "            overrides=[\n",
    "                f\"dataset={dataset}\",\n",
    "                \"compute.distributed=False\",\n",
    "                f\"model={model}\",\n",
    "                # f\"rigl.dense_allocation={dense_alloc}\",\n",
    "                f\"rigl.dense_allocation={dense_alloc}\",\n",
    "                ])\n",
    "    dotenv.load_dotenv(\"../.env\")\n",
    "    os.environ[\"IMAGE_NET_PATH\"]\n",
    "\n",
    "\n",
    "    rank=0\n",
    "    checkpoint=None\n",
    "    if checkpoint is not None:\n",
    "        run_id = checkpoint.run_id\n",
    "        optimizer_state = checkpoint.optimizer\n",
    "        scheduler_state = checkpoint.scheduler\n",
    "        pruner_state = checkpoint.pruner\n",
    "        model_state = checkpoint.model\n",
    "        cfg = checkpoint.cfg\n",
    "    else:\n",
    "        run_id, optimizer_state, scheduler_state, pruner_state, model_state = (\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    print(cfg.compute)\n",
    "    cfg.compute.distributed=False\n",
    "        \n",
    "    pl.seed_everything(cfg.training.seed)\n",
    "    use_cuda = not cfg.compute.no_cuda and torch.cuda.is_available()\n",
    "    if not use_cuda:\n",
    "        raise SystemError(\"GPU has stopped responding...waiting to die!\")\n",
    "        logger.warning(\n",
    "            \"Using CPU! Verify cfg.compute.no_cuda and \"\n",
    "            \"torch.cuda.is_available() are properly set if this is unexpected\"\n",
    "        )\n",
    "\n",
    "    if cfg.compute.distributed and use_cuda:\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "    else:\n",
    "        print(f\"loading to device rank: {rank}\")\n",
    "        device = torch.device(f\"cuda:{rank}\")\n",
    "    if not use_cuda:\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    train_loader, test_loader = get_dataloaders(cfg)\n",
    "\n",
    "    model = ModelFactory.load_model(\n",
    "        model=cfg.model.name, dataset=cfg.dataset.name\n",
    "    )\n",
    "    model.to(device)\n",
    "    if cfg.compute.distributed:\n",
    "        model = DistributedDataParallel(model, device_ids=[rank])\n",
    "    if model_state is not None:\n",
    "        try:\n",
    "            model.load_state_dict(model_state)\n",
    "        except RuntimeError:\n",
    "            model_state = checkpoint.get_single_process_model_state_from_distributed_state()\n",
    "            model.load_state_dict(model_state)\n",
    "            \n",
    "    optimizer = get_optimizer(cfg, model, state_dict=optimizer_state)\n",
    "    scheduler = get_lr_scheduler(cfg, optimizer, state_dict=scheduler_state)\n",
    "    pruner = None\n",
    "    if cfg.rigl.dense_allocation is not None:\n",
    "        T_end = get_T_end(cfg, [0 for _ in range(0,1251)])\n",
    "        if cfg.rigl.const_fan_in:\n",
    "            rigl_scheduler = RigLConstFanScheduler\n",
    "        else:\n",
    "            rigl_scheduler = RigLScheduler\n",
    "        pruner = rigl_scheduler(\n",
    "            model,\n",
    "            optimizer,\n",
    "            dense_allocation=cfg.rigl.dense_allocation,\n",
    "            alpha=cfg.rigl.alpha,\n",
    "            delta=cfg.rigl.delta,\n",
    "            static_topo=cfg.rigl.static_topo,\n",
    "            T_end=T_end,\n",
    "            ignore_linear_layers=cfg.rigl.ignore_linear_layers,\n",
    "            grad_accumulation_n=cfg.rigl.grad_accumulation_n,\n",
    "            sparsity_distribution=cfg.rigl.sparsity_distribution,\n",
    "            erk_power_scale=cfg.rigl.erk_power_scale,\n",
    "            state_dict=pruner_state,\n",
    "            filter_ablation_threshold=cfg.rigl.filter_ablation_threshold,\n",
    "            static_ablation=cfg.rigl.static_ablation,\n",
    "            dynamic_ablation=cfg.rigl.dynamic_ablation,\n",
    "            min_salient_weights_per_neuron=cfg.rigl.min_salient_weights_per_neuron,  # noqa\n",
    "            use_sparse_init=cfg.rigl.use_sparse_initialization,\n",
    "            init_method_str=cfg.rigl.init_method_str,\n",
    "            use_sparse_const_fan_in_for_ablation=cfg.rigl.use_sparse_const_fan_in_for_ablation,  # noqa\n",
    "        )\n",
    "        \n",
    "        step=0\n",
    "    return pruner, model, train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops_df(model_name, dataset):\n",
    "    df = {k:[] for k in [\"rigl.dense_allocation\", \"flops\", \"model\",]}\n",
    "    for da in [0.01, 0.05, 0.0625, 0.1, 0.2, 0.25,]:\n",
    "        print(f\"Calculating with dense_alloc == {da}\")\n",
    "        pruner, model, train_loader = get_pruner_model_loader(da, model_name, dataset)\n",
    "        model.eval()\n",
    "        for data, _ in train_loader:\n",
    "            data = data[0].to(\"cpu\").reshape(1, *data[0].shape)\n",
    "            break\n",
    "        \n",
    "        flops = FlopCountAnalysis(model.to(\"cpu\"),data)\n",
    "        total_flops = 0\n",
    "        S = pruner.S\n",
    "        names, W = get_names_and_W(model)\n",
    "        for name, counter in flops.by_module_and_operator().items():\n",
    "            if name in names:\n",
    "                if len(counter) != 1:\n",
    "                    raise ValueError(f\"Too many items found in {name}. Goodbye\")\n",
    "                f = list(counter.values())[0]\n",
    "                s = S[names.index(name)]\n",
    "                if s is None:\n",
    "                    s=1\n",
    "                total_flops += f*(1-s)\n",
    "        del model\n",
    "        del pruner\n",
    "        del train_loader\n",
    "        df[\"rigl.dense_allocation\"].append(da)\n",
    "        df[\"flops\"].append(total_flops)\n",
    "        df[\"model\"].append(model_name)\n",
    "    return pd.DataFrame(df)\n",
    "\n",
    "# df = get_flops_df(\"resnet50\", \"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_cuda': False, 'cuda_kwargs': {'num_workers': '${ oc.decode:${oc.env:NUM_WORKERS} }', 'pin_memory': True}, 'distributed': False, 'world_size': 4, 'dist_backend': 'nccl'}\n",
      "loading to device rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/models/model_factory.py:Loading model resnet50/imagenet using <function get_imagenet_resnet50 at 0x7f8ac4eb2200> with args: () and kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "p, m, l = get_pruner_model_loader(\"null\", \"resnet50\", \"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.ones(size=(1,3,224,224))\n",
    "input.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micronet_challenge.counting import *\n",
    "import torch.nn as nn\n",
    "Conv2D._fields\n",
    " \n",
    "def get_conv_op(conv: nn.Conv2d, input):\n",
    "    use_bias = True if conv.bias is not None else False\n",
    "    c_out, c_in, k_x, k_y = conv.weight.shape\n",
    "    input=torch.ones(size=(1,3,224,224))\n",
    "    return Conv2D(\n",
    "        input_size=input.shape[-1],\n",
    "        kernel_shape=(k_x, k_y, c_in, c_out),\n",
    "        strides=conv.stride,\n",
    "        use_bias=use_bias,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "def get_linear_op(linear: nn.Linear, input, use_relu_activation: bool = True):\n",
    "    c_out, c_in = m._modules['fc'].weight.shape\n",
    "    return FullyConnected(\n",
    "        kernel_shape=(c_in, c_out),\n",
    "        use_bias = True if linear.bias is not None else False,\n",
    "        activation=\"relu\" if use_relu_activation else None,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m._modules['fc'].weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating with dense_alloc == null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_cuda': False, 'cuda_kwargs': {'num_workers': '${ oc.decode:${oc.env:NUM_WORKERS} }', 'pin_memory': True}, 'distributed': False, 'world_size': 4, 'dist_backend': 'nccl'}\n",
      "loading to device rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/models/model_factory.py:Loading model resnet50/imagenet using <function get_imagenet_resnet50 at 0x7ff6dde82200> with args: () and kwargs: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fvcore.nn.flop_count.FlopCountAnalysis at 0x7ff6dc45b3a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_flops_df(model_name, dataset):\n",
    "    df = {k:[] for k in [\"rigl.dense_allocation\", \"flops\", \"model\",]}\n",
    "    # for da in [0.01, 0.05, 0.0625, 0.1, 0.2, 0.25,]:\n",
    "    for da in [\"null\",]:\n",
    "        print(f\"Calculating with dense_alloc == {da}\")\n",
    "        pruner, model, train_loader = get_pruner_model_loader(da, model_name, dataset)\n",
    "        model.eval()\n",
    "        model.to(\"cpu\")\n",
    "        for data, _ in train_loader:\n",
    "            data = data[0].to(\"cpu\").reshape(1, *data[0].shape)\n",
    "            break\n",
    "        \n",
    "        flops = FlopCountAnalysis(model.to(\"cpu\"), data)\n",
    "        return flops\n",
    "    \n",
    "flops = get_flops_df(\"resnet50\", \"imagenet\")\n",
    "flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 16 time(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.111512576"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops.total()/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1896758355328.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: FLOPS: 0.223552896\n",
      "layer1.0.conv1: FLOPS: 0.411041792\n",
      "layer1.0.conv2: FLOPS: 3.633610752\n",
      "layer1.0.conv3: FLOPS: 1.644167168\n",
      "layer1.0.downsample.0: FLOPS: 1.644167168\n",
      "layer1.1.conv1: FLOPS: 1.644167168\n",
      "layer1.1.conv2: FLOPS: 3.633610752\n",
      "layer1.1.conv3: FLOPS: 1.644167168\n",
      "layer1.2.conv1: FLOPS: 1.644167168\n",
      "layer1.2.conv2: FLOPS: 3.633610752\n",
      "layer1.2.conv3: FLOPS: 1.644167168\n",
      "layer2.0.conv1: FLOPS: 3.288334336\n",
      "layer2.0.conv2: FLOPS: 3.633610752\n",
      "layer2.0.conv3: FLOPS: 6.576668672\n",
      "layer2.0.downsample.0: FLOPS: 3.288334336\n",
      "layer2.1.conv1: FLOPS: 6.576668672\n",
      "layer2.1.conv2: FLOPS: 14.534443008\n",
      "layer2.1.conv3: FLOPS: 6.576668672\n",
      "layer2.2.conv1: FLOPS: 6.576668672\n",
      "layer2.2.conv2: FLOPS: 14.534443008\n",
      "layer2.2.conv3: FLOPS: 6.576668672\n",
      "layer2.3.conv1: FLOPS: 6.576668672\n",
      "layer2.3.conv2: FLOPS: 14.534443008\n",
      "layer2.3.conv3: FLOPS: 6.576668672\n",
      "layer3.0.conv1: FLOPS: 13.153337344\n",
      "layer3.0.conv2: FLOPS: 14.534443008\n",
      "layer3.0.conv3: FLOPS: 26.306674688\n",
      "layer3.0.downsample.0: FLOPS: 13.153337344\n",
      "layer3.1.conv1: FLOPS: 26.306674688\n",
      "layer3.1.conv2: FLOPS: 58.137772032\n",
      "layer3.1.conv3: FLOPS: 26.306674688\n",
      "layer3.2.conv1: FLOPS: 26.306674688\n",
      "layer3.2.conv2: FLOPS: 58.137772032\n",
      "layer3.2.conv3: FLOPS: 26.306674688\n",
      "layer3.3.conv1: FLOPS: 26.306674688\n",
      "layer3.3.conv2: FLOPS: 58.137772032\n",
      "layer3.3.conv3: FLOPS: 26.306674688\n",
      "layer3.4.conv1: FLOPS: 26.306674688\n",
      "layer3.4.conv2: FLOPS: 58.137772032\n",
      "layer3.4.conv3: FLOPS: 26.306674688\n",
      "layer3.5.conv1: FLOPS: 26.306674688\n",
      "layer3.5.conv2: FLOPS: 58.137772032\n",
      "layer3.5.conv3: FLOPS: 26.306674688\n",
      "layer4.0.conv1: FLOPS: 52.613349376\n",
      "layer4.0.conv2: FLOPS: 58.137772032\n",
      "layer4.0.conv3: FLOPS: 105.226698752\n",
      "layer4.0.downsample.0: FLOPS: 52.613349376\n",
      "layer4.1.conv1: FLOPS: 105.226698752\n",
      "layer4.1.conv2: FLOPS: 232.551088128\n",
      "layer4.1.conv3: FLOPS: 105.226698752\n",
      "layer4.2.conv1: FLOPS: 105.226698752\n",
      "layer4.2.conv2: FLOPS: 232.551088128\n",
      "layer4.2.conv3: FLOPS: 105.226698752\n",
      "fc: FLOPS: 0.004096\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, NamedTuple\n",
    "\n",
    "def get_op_from_module(m, input):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        return get_conv_op(m, input)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        return get_linear_op(m, input, use_relu_activation=False)  # Only 1 layer\n",
    "\n",
    "def get_names_and_ops(\n",
    "    module,\n",
    "    input: torch.Tensor,\n",
    "    target_names: Optional[List[str]]=None,\n",
    ") -> Dict[str, nn.Module]:\n",
    "    if target_names is None:\n",
    "        target_names, _ = get_names_and_W(module)\n",
    "    names_ops = {k: None for k in target_names}\n",
    "    \n",
    "    for n,m in module.named_modules():\n",
    "        if n in target_names:\n",
    "            op = get_op_from_module(m, input)\n",
    "            names_ops[n]=op\n",
    "    return names_ops\n",
    "\n",
    "\n",
    "def get_model_info(m, p):\n",
    "    names = get_names_and_ops(m, input=torch.ones(size=(1,3,224,224)))\n",
    "    # names\n",
    "\n",
    "    total_flops = 0\n",
    "    flops_dict = {n:0 for n in names}\n",
    "    if p is not None:\n",
    "        S = p.S\n",
    "    else:\n",
    "        S = [0. for _ in range(len(names))]\n",
    "    total_flops = 0\n",
    "    total_param_bits = 0\n",
    "    total_params = 0.\n",
    "    n_zeros = 0.\n",
    "    for s, (n, o) in list(zip(S, names.items())):\n",
    "        param_count, n_mults, n_adds = count_ops(o, s, param_bits=32)\n",
    "        print(f\"{n}: FLOPS: {(n_mults+n_adds)/1e9}\")\n",
    "        k_shape = o.kernel_shape\n",
    "        total_param_bits += param_count\n",
    "        total_flops += n_mults + n_adds\n",
    "        n_param = np.prod(k_shape)\n",
    "        total_params += n_param\n",
    "        n_zeros += int(n_param * s)\n",
    "    return total_flops, total_param_bits, n_zeros / total_params\n",
    "\n",
    "total_flops, params, global_sparsity = get_model_info(m, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_o = get_names_and_ops(m, torch.ones(size=(1,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1896.758355328"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_flops/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1': Conv2D(input_size=224, kernel_shape=(7, 7, 3, 64), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.0.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 64, 64), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.0.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 64, 64), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.0.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 64, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.0.downsample.0': Conv2D(input_size=224, kernel_shape=(1, 1, 64, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.1.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 64), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.1.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 64, 64), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.1.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 64, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.2.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 64), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.2.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 64, 64), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer1.2.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 64, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.0.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.0.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 128, 128), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.0.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 128, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.0.downsample.0': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 512), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.1.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.1.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 128, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.1.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 128, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.2.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.2.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 128, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.2.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 128, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.3.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.3.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 128, 128), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer2.3.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 128, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.0.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.0.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 256, 256), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.0.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 1024), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.0.downsample.0': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 1024), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.1.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.1.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 256, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.1.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 1024), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.2.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.2.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 256, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.2.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 1024), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.3.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.3.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 256, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.3.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 1024), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.4.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.4.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 256, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.4.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 1024), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.5.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.5.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 256, 256), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer3.5.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 256, 1024), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.0.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.0.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 512, 512), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.0.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 2048), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.0.downsample.0': Conv2D(input_size=224, kernel_shape=(1, 1, 1024, 2048), strides=(2, 2), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.1.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 2048, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.1.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 512, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.1.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 2048), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.2.conv1': Conv2D(input_size=224, kernel_shape=(1, 1, 2048, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.2.conv2': Conv2D(input_size=224, kernel_shape=(3, 3, 512, 512), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'layer4.2.conv3': Conv2D(input_size=224, kernel_shape=(1, 1, 512, 2048), strides=(1, 1), padding='valid', use_bias=False, activation='relu'),\n",
       " 'fc': FullyConnected(kernel_shape=(2048, 1000), use_bias=True, activation=None)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1896.758355328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_flops/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25503912.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25557032"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=0\n",
    "for p in m.parameters():\n",
    "    total+=p.numel()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child.__dict__[\"_modules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99517038.63962848"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_idx = 0\n",
    "total_flops = 0\n",
    "modules_to_ignore = []\n",
    "S = pruner.S\n",
    "names, W = get_names_and_W(model)\n",
    "for name, counter in flops.by_module_and_operator().items():\n",
    "    if name in names:\n",
    "        if len(counter) != 1:\n",
    "            raise ValueError(\"?\")\n",
    "        f = list(counter.values())[0]\n",
    "        s = S[names.index(name)]\n",
    "        if s is None:\n",
    "            s=1\n",
    "        total_flops += f*(1-s)\n",
    "\n",
    "total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.089284608"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4089284608/1e9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4089284608/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_allocation': {0: 0.01,\n",
       "  1: 0.05,\n",
       "  2: 0.0625,\n",
       "  3: 0.1,\n",
       "  4: 0.2,\n",
       "  5: 0.25,\n",
       "  6: 0.3,\n",
       "  7: 0.4,\n",
       "  8: 0.5},\n",
       " 'parameters': {0: 274072,\n",
       "  1: 1289784,\n",
       "  2: 1611784,\n",
       "  3: 2571656,\n",
       "  4: 5113920,\n",
       "  5: 6391944,\n",
       "  6: 7663032,\n",
       "  7: 10222975,\n",
       "  8: 12774296},\n",
       " 'dense_params': {0: 25557032,\n",
       "  1: 25557032,\n",
       "  2: 25557032,\n",
       "  3: 25557032,\n",
       "  4: 25557032,\n",
       "  5: 25557032,\n",
       "  6: 25557032,\n",
       "  7: 25557032,\n",
       "  8: 25557032}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(params).to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8e7aa388760f1ed0054a801e8f4bc0d2f712d90d0781f57f52c8b826e4e7fab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
