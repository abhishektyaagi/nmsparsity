defaults:
  - dataset: cifar10
  - model: wide_resnet22
  - _self_

experiment:
  comment: sweep
  name: ${model.name}_${dataset.name}_${experiment.comment}
  sweep: True

paths:
  data_folder: /home/condensed-sparsity/data
  artifacts: /home/condensed-sparsity/artifacts
  logs: /home/condensed-sparsity/logs

rigl: 
  dense_allocation: 0.5
  # percentage of dense parameters allowed. if None, pruning will not 
  # be used. must be on the interval (0, 1]
  dense_allocation: 0.1
  # delta param for pruning
  delta: 100
  # number of gradients to accumulate before scoring for rigl",
  grad_accumulation_n: 1
  # alpha param for pruning for cosine decay in determining how many connections to prune/regrow
  alpha: 0.3
  # if 1, use random sparsity topo and remain static"
  static_topo: 0
  # If True, use const_fan_in scheduler.
  const_fan_in: False
  # Define layer-wise sparsity distribution. Options include `uniform` & `erk`
  sparsity_distribution: erk
  # Power scale for ERK distribution
  erk_power_scale: 1.0

training:
  dry_run: False
  # quickly check a single pass",
  batch_size: 128
  # input batch size for testing (default: 1000)
  test_batch_size: 1000
  # number of epochs to train (default: 250)
  epochs: 250
  # number of epochs to train (default: 14)",
  seed: 1
  # random seed (default: 1)",
  log_interval: 10
  # For Saving the current Model
  save_model: True

  ## Optimization
  optimizer: sgd # "sgd", "adamw"
  weight_decay: 5.0e-4  # 5.0e-4 -> rigl value
  # L2 Regularization for optimizer
  momentum: 0.9  # 0.9 -> rigl value
  # Momentum coefficient for SGD optimizer

  ## Scheduler
  scheduler: step_lr # cosine_annealing_with_warm_up, step_lr_with_warm_up
  lr: 0.1
  # Learning rate after warmup (default: 0.01)",
  init_lr: 0.000001
  # Learning rate for first epoch of warm up
  warm_up_steps: 5
  # Number of epochs to warm up for linear warm ups
  gamma: 0.2
  # Learning rate step gamma
  step_size: 30000
  # Step size to use in learning rate scheduler if StepLR is used.

compute:
  # disables CUDA training",
  no_cuda: False
  cuda_kwargs: {"num_workers": 1, "pin_memory": True, "shuffle": True}

wandb:
  project: condensed-rigl
  entity: condensed-sparsity
  start_method: thread

hydra:  
  run:
    dir: ${paths.logs}
