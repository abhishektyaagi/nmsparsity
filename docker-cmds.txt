# Hacks until we have compose installed:
# docker build --file ./Dockerfile -t rigl-agent --shm-size=16gb .
# docker create --cpus 12 --memory 30G --env-file .env --gpus '"device=1"' -i -t rigl-sweep-agent:latest
# docker create --env-file .env --gpus '"device=0"' -i -t rigl-sweep-agent:latest
# docker run --cpus 12 --memory 64G --env-file .env --gpus '"device=0"' -i -t --name gpu-0 rigl-agent:latest
# docker run --cpus 12 --memory 30G --env-file .env --gpus '"device=1"' -i -t -d --name gpu-1 rigl-sweep-agent:latest
# docker run --cpus 12 --memory 30G --env-file .env --gpus '"device=2"' -i -t --name gpu-2 rigl-sweep-agent:latest
# docker run --cpus 12 --memory 30G --env-file .env --gpus '"device=3"' -i -t --name gpu-3 rigl-sweep-agent:latest
# docker run -itd --env-file ./.env --mount source=/home/mike/condensed-sparsity,target=/home/condensed-sparsity,type=bind --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind --gpus all --shm-size 16G --name cond-rigl-dev ac605476a22f


docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=0"' \
  --shm-size 16G \
  --name gpu-0 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/gu6x4hlp

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=1"' \
  --shm-size 16G \
  --name gpu-1 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/gu6x4hlp

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=2"' \
  --shm-size 16G \
  --name gpu-2 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/gu6x4hlp

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=3"' \
  --shm-size 16G \
  --name gpu-3 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/gu6x4hlp

  
python ./train_rigl.py \
  rigl.dense_allocation=0.01 \
  rigl.delta=1600 \
  training.batch_size=256 \
  training.max_steps=512000 \
  training.lr=0.1 \
  compute.world_size=2 \
  rigl.grad_accumulation_n=16 \
  rigl.min_salient_weights_per_neuron=10

python ./train_rigl.py \
  dataset=cifar10 \
  model=resnet18 \
  rigl.dense_allocation=0.01 \
  rigl.delta=100 \
  rigl.grad_accumulation_n=1 \
  rigl.min_salient_weights_per_neuron=0.05 \
  training.batch_size=128 \
  training.max_steps=null \
  training.weight_decay=5.0e-4 \
  training.label_smoothing=0 \
  training.lr=0.1 \
  training.epochs=250 \
  training.warm_up_steps=0 \
  training.scheduler=step_lr \
  training.step_size=77 \
  training.gamma=0.2 \
  compute.distributed=False



docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=0"' \
  --shm-size 16G \
  --name gpu-0 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  python ./train_rigl.py \
  dataset=cifar10 \
  model=resnet18 \
  rigl.dense_allocation=0.01 \
  rigl.delta=100 \
  rigl.grad_accumulation_n=1 \
  rigl.min_salient_weights_per_neuron=1 \
  training.batch_size=128 \
  training.max_steps=null \
  training.weight_decay=5.0e-4 \
  training.label_smoothing=0 \
  training.lr=0.1 \
  training.epochs=250 \
  training.warm_up_steps=0 \
  training.scheduler=step_lr \
  training.step_size=77 \
  training.gamma=0.2 \
  compute.distributed=False \
  rigl.use_sparse_initialization=True \
  rigl.init_method_str=grad_flow_init \
  training.seed=42

# Imagenet
python ./train_rigl.py \
dataset=imagenet \
model=resnet50 \
rigl.dense_allocation=0.01 \
rigl.delta=800 \
rigl.grad_accumulation_n=8 \
rigl.min_salient_weights_per_neuron=1 \
training.batch_size=512 \
training.max_steps=256000 \
training.weight_decay=0.0001 \
training.label_smoothing=0.1 \
training.lr=0.2 \
training.epochs=104 \
training.warm_up_steps=5 \
training.scheduler=step_lr_with_warm_up \
training.step_size=[30,70,90] \
training.gamma=0.1 \
compute.distributed=True \
rigl.use_sparse_initialization=True \
rigl.init_method_str=grad_flow_init
